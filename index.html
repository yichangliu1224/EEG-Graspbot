<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EEG-based robotic grasping and placement with hybrid visual and motor imagery">
  <meta name="keywords" content="EEG, BCI, motor imagery, visual imagery, robotics, grasping, placement">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Robotic Grasping and Placement Controlled by EEG-Based Hybrid Visual and Motor Imagery</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  
  <link rel="icon" href="./static/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <!-- Title -->
          <!-- <h1 class="title is-1 publication-title">            
            <img src="./static/icon.png" id="logo" width="5%"><span style="background: linear-gradient(to right,  indigo, skyblue, violet, indigo, violet); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">
                HOP
            </span>: <span style="color: skyblue;">H</span>eterogeneous T<span style="color: skyblue;">o</span>pology-based Multimodal Entanglement for Co-S<span style="color: skyblue;">p</span>eech Gesture Generation</h1>
            <img src="./assets/images/logo.png" id="logo" width="5%" alt="Project Logo">
            Robotic Grasping and Placement Controlled by EEG-Based Hybrid Visual and Motor Imagery
            </h1> -->

            <h1 class="title is-1 publication-title" style="line-height:1.15;">
              <img src="./static/icon.png" id="logo" width="5%" alt="Project Logo" style="vertical-align:middle; margin-right:10px;">
              <span style="background: linear-gradient(to right, indigo, skyblue, violet); -webkit-background-clip:text; -webkit-text-fill-color:transparent;">
                Robotic Grasping and Placement Controlled by EEG-Based Hybrid Visual and Motor Imagery
              </span>
            </h1>

            <h4 class="title publication-title" style="font-size: 1.5rem;">ICRA 2026</h4>


          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Yichang Liu</a><sup>1</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://star-uu-wang.github.io/" target="_blank" rel="noopener">Tianyu Wang</a><sup>2,4</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ziyiye.cn/" target="_blank" rel="noopener">Ziyi Ye</a><sup>3*</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="#" onclick="return false;">Yawei Li</a><sup>5</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="#" onclick="return false;">Yugang Jiang</a><sup>3</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="#" onclick="return false;">Shouyan Wang</a><sup>1*</sup>
            </span>
            <span class="author-block">
              <a href="https://yanweifu.github.io/" target="_blank" rel="noopener">Yanwei Fu</a><sup>2,4</sup>,&nbsp;
            </span>
          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Science and Technology for Brain-inspired Intelligence (ISTBI), Fudan University&nbsp; &nbsp;</span>
            <span class="author-block"><sup>2</sup>School of Data Science, Fudan University&nbsp; &nbsp;</span>
            <span class="author-block"><sup>3</sup>Institute of Trustworthy Embodied AI, Fudan University&nbsp; &nbsp;</span>
            <span class="author-block"><sup>4</sup>Shanghai Innovation Institute&nbsp; &nbsp;</span>
            <span class="author-block"><sup>5</sup>ETH Zürich</span>


          </div>

          <p>
            <sup>*</sup> corresponding authors &nbsp;
          </p>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#" onclick="return false;"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" onclick="return false;"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              </span>
              <!-- Code Link. href="https://github.com/yichangliu1224/EEG-Graspbot" target="_blank" rel="noopener" -->
              <span class="link-block">
                <a href="#" onclick="return false;"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
<!--         <h2 class="title is-3">Abstract</h2> -->
        <h2 class="title is-3" style="background: linear-gradient(to right,  indigo, indigo, skyblue,indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;"> Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a framework that integrates EEG-based visual and motor imagery (VI/MI) with robotic control to enable real-time, intention-driven grasping and placement. 
            Motivated by the promise of BCI-driven robotics to enhance human-robot interaction, this system bridges neural signals with physical control by deploying offline-pretrained decoders in a zero-shot manner within an online streaming pipeline. 
            This establishes a dual-channel intent interface that translates visual intent into robotic actions, with VI identifying objects for grasping and MI determining placement poses, enabling intuitive control over both what to grasp and where to place. 
            The system operates solely on EEG via a cue-free imagery protocol, achieving integration and online validation. 
            Implemented on a Base robotic platform and evaluated across diverse scenarios, including occluded targets or varying participant postures, the system achieves online decoding accuracies of 40.23% (VI) and 62.59% (MI), with an end-to-end task success rate of 20.88%. 
            These results demonstrate that high-level visual cognition can be decoded in real time and translated into executable robot commands, bridging the gap between neural signals and physical interaction, and validating the flexibility of a purely imagery-based BCI paradigm for practical human–robot collaboration.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="background: linear-gradient(to right,  indigo, indigo, skyblue,indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;"> Overview</h2>
        <div class="publication-image">
          <img src="./assets/images/figure.png" alt="Overview Figure" style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p><b>High-level cognitive control pipeline for EEG-based robotic grasp-and-place.</b></p>
          <p>
            The system integrates offline-trained VI/MI decoders into an online streaming pipeline: VI decoding determines grasping intent (object selection),
            while MI decoding determines placement intent (target pose/position). The resulting dual-channel commands drive a robotic arm to execute the grasp-and-place task in real-world settings, demonstrating seamless mapping from high-level visual cognition to physical manipulation.
          </p>
          <!-- <ol>
            <li><b>Offline EEG data collection:</b> visual imagery (VI) and motor imagery (MI) EEG data are collected and labeled to train visual and motor decoders.</li>
            <li><b>Online EEG acquisition:</b> a dual-channel system is deployed in which VI-EEG is decoded into grasping intentions and MI-EEG determines placement positions, enabling real-time task-level command generation.</li>
            <li><b>Robotic control:</b> the trained decoders drive a robotic arm to perform the grasp-and-place task in real-world settings, demonstrating seamless mapping from high-level visual cognition to physical manipulation.</li>
          </ol> -->
        </div>
      </div>
    </div>


    <!-- Paper image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="background: linear-gradient(to right,  indigo, indigo, skyblue,indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;"> Data Collection</h2>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Participants and Experimental Setup</h3>
        <div class="content has-text-justified">
          <p>
            <b>Participants.</b> Five healthy graduate students (2 females; age 23–30 years; M = 25.4, SD = 2.97) participated in this study.
            All five completed the offline experiments, and four further engaged in the online testing.
          </p>
          <p>
            <b>Offline tasks.</b> Offline data collection comprised visual perception (VP), visual imagery (VI), and motor imagery (MI) tasks.
          </p>
          <p>
            <b>EEG acquisition.</b> EEG was recorded using a 64-channel Neuracle system (59 scalp EEG, 2 mastoid, 2 EOG, 1 ECG) at a sampling rate of 1000 Hz.
          </p>
          <p>
            <b>Robotics platform.</b> We employed a KINOVA GEN2 robot as the embodied platform for systematic interaction.
            Two Intel RealSense D435 cameras served as the primary sensors of the robotic perception system.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">VI/MI Paradigms</h3>
    
        <div class="publication-image">
          <img src="./assets/images/paradigm.png" alt="VI/MI Paradigms"
          style="display:block; margin:0 auto; width:65%; max-width:850px; height:auto;">
        </div>
    
        <div class="content has-text-justified">
          <p>
            We design a cue-free imagery protocol with three offline tasks—visual perception (VP), visual imagery (VI), and motor imagery (MI).
            During online control, VI is decoded to infer grasping intent (what to grasp), while MI is decoded to infer placement intent (where to place),
            enabling a dual-channel interface for real-time grasp-and-place.
          </p>
        </div>
      </div>
    </div>

    <!-- Offline Results (3 images horizontally) -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Offline Results</h3>

        <div class="content has-text-justified">
          <p>
            Offline results for three tasks: (a) Visual Perception, (b) Visual Imagery, and (c) Motor Imagery.
            For each task, accuracies are reported per participant across frequency conditions.
          </p>
        </div>

        <div class="columns is-multiline is-centered">
          <div class="column is-one-third">
            <figure>
              <img src="./assets/images/grouped_accuracy_scatter-VP.png" alt="Offline VP Results" style="width: 100%; height: auto;">
              <figcaption>(a) Visual Perception (VP)</figcaption>
            </figure>
          </div>

          <div class="column is-one-third">
            <figure>
              <img src="./assets/images/grouped_accuracy_scatter-VI.png" alt="Offline VI Results" style="width: 100%; height: auto;">
              <figcaption>(b) Visual Imagery (VI)</figcaption>
            </figure>
          </div>

          <div class="column is-one-third">
            <figure>
              <img src="./assets/images/grouped_accuracy_scatter-MI.png" alt="Offline MI Results" style="width: 100%; height: auto;">
              <figcaption>(c) Motor Imagery (MI)</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- =========================
      ONLINE: Real-time Robotic Control
    ========================= -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;"> Online: Real-time Robotic Control </h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
    
        <div class="content has-text-justified">
          <p>
            <b>Online system results.</b>
            <b>Top:</b> Base demo performance per subject on online VI/MI tasks across model/frequency settings
            (e.g., 63.64 (RGNN/40) denotes accuracy using the RGNN model at 40 Hz).
            For each subject, we report the top-2 online accuracies and the overall mean.
            <b>Bolded values</b> indicate the row maximum, while <u>underlined values</u> mark the second highest.
            <b>Middle:</b> Overall online system accuracy, with online VI/MI accuracy representing the mean top-2 accuracies across all subjects.
            <b>Bottom:</b> Overall system runtime statistics.
          </p>
        </div>
    
        <!-- =========================
          Top table
        ========================= -->
        <h3 class="title is-6">Online Base Demonstration Results</h3>
        <div class="table-container">
          <table class="table is-striped is-bordered is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th rowspan="2">Subject</th>
                <th colspan="3">VI-online Task</th>
                <th colspan="3">MI-online Task</th>
              </tr>
              <tr>
                <th>Top1 Acc. (%)</th>
                <th>Top2 Acc. (%)</th>
                <th>Overall Avg. (%)</th>
                <th>Top1 Acc. (%)</th>
                <th>Top2 Acc. (%)</th>
                <th>Overall Avg. (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>01</td>
                <td>63.64 (RGNN/40)</td>
                <td>44.44 (RGNN/60)</td>
                <td><b>47.14</b></td>
                <td>75.00 (RGNN/60)</td>
                <td>60.00 (MLP/40)</td>
                <td><b>61.67</b></td>
              </tr>
              <tr>
                <td>02</td>
                <td>50.00 (EEGNet/100)</td>
                <td>39.13 (MLP/100)</td>
                <td><u>40.82</u></td>
                <td>65.22 (MLP/100)</td>
                <td>60.00 (MLP/40)</td>
                <td>58.41</td>
              </tr>
              <tr>
                <td>03</td>
                <td>50.00 (MLP/60)</td>
                <td>33.67 (RGNN/60)</td>
                <td>38.89</td>
                <td>60.00 (RGNN/100)</td>
                <td>53.85 (RGNN/60)</td>
                <td>54.62</td>
              </tr>
              <tr>
                <td>04</td>
                <td>37.50 (RGNN/100)</td>
                <td>33.67 (MLP/60)</td>
                <td>34.72</td>
                <td>66.67 (MLP/60)</td>
                <td>60.00 (MLP/40)</td>
                <td><u>60.00</u></td>
              </tr>
            </tbody>
          </table>
        </div>
    
        <hr>
    
        <!-- =========================
          Middle table
        ========================= -->
        <h3 class="title is-6">Online System Accuracy</h3>
        <div class="table-container">
          <table class="table is-striped is-bordered is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Online VI avg Acc. (%)</th>
                <th>Query success rate (%)</th>
                <th>Online MI avg Acc. (%)</th>
                <th>Place success rate (%)</th>
                <th>System Accuracy (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>40.23</b></td>
                <td>76.11</td>
                <td><b>62.59</b></td>
                <td>100</td>
                <td><b>20.88</b></td>
              </tr>
            </tbody>
          </table>
        </div>
    
        <hr>
    
        <!-- =========================
          Bottom table
        ========================= -->
        <h3 class="title is-6">System Operation Times (s)</h3>
        <div class="table-container">
          <table class="table is-striped is-bordered is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Prepare</th>
                <th>VI Task</th>
                <th>VI Data Proc.</th>
                <th>VI Infer</th>
                <th>MI Task</th>
                <th>MI Data Proc.</th>
                <th>MI Infer</th>
                <th>Robot Exec.</th>
                <th>System Total</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>6.010</td>
                <td>15.000</td>
                <td>0.639</td>
                <td><b>8.191</b></td>
                <td>15.000</td>
                <td>0.524</td>
                <td><b>8.000</b></td>
                <td><b>54.872</b></td>
                <td><b>107.266</b></td>
              </tr>
            </tbody>
          </table>
        </div>

        <hr>
    
      </div>
    </div>

    <!-- Online Demo (Extended) figure -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Extended Online Demos</h3>

        <div class="publication-image">
          <img src="./assets/images/online_demo.png" alt="Online demos for extended scenarios" 
          style="display:block; margin:0 auto; width:65%; max-width:900px; height:auto;">
        </div>

        <div class="content has-text-justified">
          <p>
            <b>Online Demos for extended application scenarios.</b>
            <p>
              <b>(a) Hidden Object Interaction:</b> Using only VI-EEG, the subject guides the system to identify and reveal a hidden object, demonstrating decoding without visual cues.
            </p>
            <p>
              <b>(b) Direct Human–Robot Interaction:</b> By imagining the object and controlling hand movements through VI/MI, the subject interacts seamlessly with the robot, which places the object directly into their hand.
            </p>
          </p>
        </div>
      </div>
    </div>

    <!-- Online Demo Videos (1 wide + 4 small 2x2) -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Online Demo Videos</h3>

        <h4 class="title is-5" style="margin-top: 10px;">Full demonstration</h4>
        <div class="content">
          <video controls preload="metadata" playsinline width="100%" style="border-radius: 12px;">
            <source src="./assets/videos/online_full.mp4" type="video/mp4">
          </video>
        </div>

        <h4 class="title is-5" style="margin-top: 18px;">Supplementary clips</h4>
        <div class="columns is-multiline is-centered">
          <div class="column is-half">
            <video controls preload="metadata" playsinline width="100%" style="border-radius: 12px;">
              <source src="./assets/videos/supp_1.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">Clip 1</p>
          </div>

          <div class="column is-half">
            <video controls preload="metadata" playsinline width="100%" style="border-radius: 12px;">
              <source src="./assets/videos/supp_2.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">Clip 2</p>
          </div>

          <div class="column is-half">
            <video controls preload="metadata" playsinline width="100%" style="border-radius: 12px;">
              <source src="./assets/videos/supp_3.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">Clip 3</p>
          </div>

          <div class="column is-half">
            <video controls preload="metadata" playsinline width="100%" style="border-radius: 12px;">
              <source src="./assets/videos/supp_4.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">Clip 4</p>
          </div>
        </div>
      </div>
    </div>
  </div>



</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{cheng2025hop,
  title     = {HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation},
  author    = {Cheng, Hongye and Wang, Tianyu and Shi, Guangsi and Zhao, Zexing and Fu, Yanwei},
  journal   = {arXiv preprint arXiv:2503.01175},
  year      = {2025}
}</code></pre>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:center">
            Please contact Yichang Liu: <a href="mailto:ycliu24@m.fudan.edu.cn">ycliu24@m.fudan.edu.cn</a> or Professor Yanwei Fu:
            <a href="mailto:yanweifu@fudan.edu.cn">yanweifu@fudan.edu.cn</a> for feedback and questions.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
